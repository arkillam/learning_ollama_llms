
I have been learning how to use Python, Ollama and Postgres to work with LLMs, build RAG applications, use agents and tools etc.  I am writing what I do as I go, for my own future reference and in case anyone else is interested.  I do most of my work in Java, so you will see some basic Python refreshing and learning happening at the same time.  I did not take a langchain course first, and am not chaining the calls as I probably should - that will come later.

My setup:
 - Windows 11, running on an AMD Ryzen 5 5600GT with no dedicated graphics card and 64 GB of RAM (a mid-range computer, but weak on graphics and with a silly amount of RAM)
 - Ollama, downloaded from https://ollama.com/
 - PostgresSQL Community v16, downloaded from https://www.postgresql.org/download/windows/
 - Python 3, from https://www.python.org/
 - VS Code, from https://code.visualstudio.com/

Some notes on how I set up Postgres, especially the pgvector extension, can be found in postgres_notes.txt.

I set up my sandbox in C:\Coding\ws_ai - the contents are in this repository, except for database.ini, which holds my credentials and you will have to create with your credentials.

To run the Python virtual environment, I had to set the execution policy first.  I do this every time, in the console, with scope that only lasts for this one time.
    Set-ExecutionPolicy -ExecutionPolicy AllSigned -Scope Process

I ran these commands to set up a Python virtual environment, to make managing dependencies easier.
    python -m venv C:\Coding\ws_ai\sandbox\venv (to create)
    .\venv\Scripts\activate (to activate the environment, run from C:\Coding\ws_ai\sandbox)

I use "pip install" to install libraries that I use, and "pip freeze > requirements.txt" to capture them.  To install them all at once, run:
    pip install -r requirements.txt

I wrote postgres_demo.py to make sure I could use my Postgres setup, and to refresh on how to do basic database interactions with Python.

I get the impression langchain is meant to be used as one eye-pleasing chain of calls.  I may not get to that point, probably leaving it in chunks to make debugging and comprehension easier.  My first script is langchain_embed_single_file.py, which chunks up a large text file, creates embeddings, and stores the content in the database for future queries.

Postgres:

I wrote postgres_demo.py to make sure I had a working Postgres database and could query it.

Embeddings:

Different embedding combinations (model, chunk size, overlap size, and the tables to store the embeddings in) are provided via useful_stuff.py's def embedding_setups().  You can see how I use it in embed_multiple_files.py.

I tried generating embeddings with various general-purpose models, and found it painfully slow. I am not only using "embedding" models from Ollama's website.  You can see my notes on each in the comments at the top of embed_multiple_files.py.

Some embedding models break if I pass in chunk sizes that are too large.

I have various model and chunk size setups now, to test for efficacy with my RAG application.

RAG Attempts 1-4:

I have four of these so far.  I learned about context length (sometimes calling it context window), limiting what I passed in to fit in that space, and how to find chunks to generate good responses.  I tested lots of models, lots of chunk sizes (re-running embed_multiple_files_nomic.py many times), and ended up with a attempt #3 working well enough that I am satisfied with the results for now.  I should probably learn how to use langchain as it is intended, with calls chained, but am pausing to work on this to learn about agents and tools.

The fourth attempt includes pulling embedding and generative model information from lists provided by functions in useful_stuff.py.  This makes swapping models much easier.

Agents & Tools:

"tools_01.py" does what it says in the comments at the top of that file.  Probably best just to read them there.

Here is a list of provided integration tools for langchain:  https://python.langchain.com/docs/integrations/tools/

"search_duckduckgo.py" shows two tools for searching Duck Duck Go (only one of them seems useful to me, but I may be missing the value of the other option)

