
I have been learning how to use Python, Ollama and Postgres to work with LLMs, build RAG applications, use agents and tools etc.  I am writing what I do as I go, for my own future reference and in case anyone else is interested.  I do most of my work in Java, so you will see some basic Python refreshing and learning happening at the same time.  I did not take a langchain course first, and am not chaining the calls as I probably should - that will come later.

My setup:
 - Windows 11, running on an AMD Ryzen 5 5600GT with no dedicated graphics card and 64 GB of RAM (a mid-range computer, but weak on graphics and with a silly amount of RAM)
 - Ollama, downloaded from https://ollama.com/
 - PostgresSQL Community v16, downloaded from https://www.postgresql.org/download/windows/
 - Python 3, from https://www.python.org/
 - VS Code, from https://code.visualstudio.com/

Some notes on how I set up Postgres, especially the pgvector extension, can be found in postgres_notes.txt.

I set up my sandbox in C:\Coding\ws_ai - the contents are in this repository, except for database.ini, which holds my credentials and you will have to create with your credentials.

To run the Python virtual environment, I had to set the execution policy first.  I do this every time, in the console, with scope that only lasts for this one time.
    Set-ExecutionPolicy -ExecutionPolicy AllSigned -Scope Process

I ran these commands to set up a Python virtual environment, to make managing dependencies easier.
    python -m venv C:\Coding\ws_ai\sandbox\venv (to create)
    .\venv\Scripts\activate (to activate the environment, run from C:\Coding\ws_ai\sandbox)

I use "pip install" to install libraries that I use, and "pip freeze > requirements.txt" to capture them.  To install them all at once, run:
    pip install -r requirements.txt

I wrote postgres_demo.py to make sure I could use my Postgres setup, and to refresh on how to do basic database interactions with Python.

I get the impression langchain is meant to be used as one eye-pleasing chain of calls.  I may not get to that point, probably leaving it in chunks to make debugging and comprehension easier.  My first script is langchain_embed_single_file.py, which chunks up a large text file, creates embeddings, and stores the content in the database for future queries.

Postgres:

I wrote postgres_demo.py to make sure I had a working Postgres database and could query it.

Embeddings:

I did this first.  The "embed_" scripts use langchain, but are probably not great examples of the right way to use langchain (again, did not take a course in that first).  I wrot the single file one first, then the multiple files using nomic.  I am happy with these.  I found that deepseek embeddings did not lead to good search matches, and did not finish the threaded embedding script - one at a time seems to tax my PC's CPU enough.  Embedding a PDF worked, but the content was scattered, with weird spaces breaking up words, so I stopped working with PDFs quickly, though the code does work.

RAG Attempts 1-3:

I have three of these so far.  I learned about context length (sometimes calling it context window), limiting what I passed in to fit in that space, and how to find chunks to generate good responses.  I tested lots of models, lots of chunk sizes (re-running embed_multiple_files_nomic.py many times), and ended up with a attempt #3 working well enough that I am satisfied with the results for now.  I should probably learn how to use langchain as it is intended, with calls chained, but am pausing to work on this to learn about agents and tools.

Agents & Tools:

"tools_01.py" does what it says in the comments at the top of that file.  Probably best just to read them there.

Here is a list of provided integration tools for langchain:  https://python.langchain.com/docs/integrations/tools/

"search_duckduckgo.py" shows two tools for searching Duck Duck Go (only one of them seems useful to me, but I may be missing the value of the other option)

