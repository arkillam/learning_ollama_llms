
import psycopg2
import useful_stuff

from datetime import datetime
from langchain_community.document_loaders import TextLoader
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from useful_stuff import logger

# I am going to start using different tables for embeddings generated by different models, to compare results

# This script uses nomic-embed-text to generate embeddings, and stores them in a table called andrew.nomic_embeddings

# I bumped up the chunk size from 500/50 to 4000/400 because I think data is often left out of subsequent chunks (based on some tests)

# using nomic-embed-text to generate embeddings; it has a 8192 context-length (https://www.nomic.ai/blog/posts/nomic-embed-text-v1)
# produces vectors of 768 dimensions
embedding_model = 'nomic-embed-text'
chunkSize = 4000
overlapAmount = 400

# the separators used to break text up into chunks
#separators = ['\n\n','\n',' ',''] # this one is the default
#separators = ['\nChapter|\nAppendix','\nTable', '\n\n','\n',' ','']

def embedTextFile(filename, conn, embedder, text_splitter):
    """
    Reads in a text file, chunks it up, and embeds it in the Postgres database.
    :param filename: the full path to the file being loaded in
    :param conn: database connection
    :param embedder: an OllamaEmbeddings object
    :param text_splitter: a RecursiveCharacterTextSplitter object
    """
    logger("called for " + filename)

    # returns a list of langchain Document objects (in this case, one)
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html
    # https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document
    # each document has a page_content field, and a metadata field
    raw_documents = TextLoader(filename).load()
    logger('file read in')

    # chunk it up
    documents = text_splitter.split_documents(raw_documents)
    howmanyParts = len(documents)
    logger('document split up into {} parts'.format(howmanyParts))

    # open a cursor to perform operatoins
    cur = conn.cursor()

    # delete existing embeddings for this file
    deleteQuery = "delete from andrew.nomic_embeddings where source = '{}'".format(filename)
    logger("deleteQuery -> " + deleteQuery)
    cur.execute(deleteQuery)
    conn.commit()

    short_name = filename.split('\\')[-1]

    index = 0;
    for doc in documents:

        single_vector = embedder.embed_query(doc.page_content)

        #print(doc.page_content)

        logger("{} -> {} of {} -> text size {} vector size {}".format(short_name, index, howmanyParts, len(doc.page_content), len(single_vector)))

        insertQuery = "insert into andrew.nomic_embeddings (source, content, embedding) values (%s, %s, %s)"
        data = (filename, doc.page_content, single_vector)
        cur.execute(insertQuery, data)

        index += 1

    cur.close()
    conn.commit() # alternately, could set conn.autocommit = True above
    logger('database changes committed')

# connect to the database
config = useful_stuff.load_confg()
conn = psycopg2.connect(**config)
logger ('connected to database')

embedder = OllamaEmbeddings(model=embedding_model, base_url='http://localhost:11434')

#text_splitter = RecursiveCharacterTextSplitter(separators=separators, is_separator_regex=True,chunk_size = chunkSize, chunk_overlap  = overlapAmount)
text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunkSize, chunk_overlap  = overlapAmount)

# list of files to embed; I find putting them in an array is a bit easier to read
# (I have a set of D&D rules from the 1990s I bought in text form ... useful because I can ask questions from a complex set of rules and know if I received correct answers)
filesToEmbed = [
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\complete_fighter.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\complete_thief.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\complete_wizard.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\dmg.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\monster_manual.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\phb.txt',    
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\po_combat_tactics.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\po_skills_powers.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\po_spells_magic.txt',
    'D:\\Files\\RPG Books\\ADnD 2E RTFs\\tom.txt'
]

for f in filesToEmbed:
    embedTextFile(filename=f,conn=conn,embedder=embedder,text_splitter=text_splitter)

conn.close()
logger ('closed database connection')
